{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c7f4904",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c4a32be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=\"AIzaSyC1-KxZ89Jdt741SCrvkI79Ut8n5jEv7IE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfd84ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(\"gemini-2.5-flash-preview-04-17\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "113bff22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's break down \"Model Context Protocol.\" It's not a single, formal, standardized protocol like HTTP or TCP/IP. Instead, it refers to the *set of conventions, techniques, and architectural patterns* used to manage and provide information (the \"context\") to a large language model (LLM) so it can generate a relevant and coherent response.\n",
      "\n",
      "Think of it as the *system* or *methodology* for preparing the input for an LLM, specifically considering its limitations and how it processes information.\n",
      "\n",
      "Here are the key aspects covered by the \"Model Context Protocol\":\n",
      "\n",
      "1.  **The Context Window Limit:** LLMs have a finite memory or \"context window,\" measured in tokens, that they can process at any one time. The protocol acknowledges this limit and dictates how to handle information that exceeds it.\n",
      "2.  **Structuring Input (The Prompt):** This is perhaps the most visible part. The protocol defines *how* information is structured within the input prompt given to the model. Common structures include:\n",
      "    *   **System Message:** Instructions, role-playing definitions, or constraints for the model's overall behavior throughout the interaction.\n",
      "    *   **User Messages:** The user's input turns in a conversation.\n",
      "    *   **Assistant Messages:** The model's previous responses in a conversation. Including these is crucial for maintaining conversational history.\n",
      "    *   **Other Contextual Information:** Relevant documents, examples, specific data points inserted into the prompt.\n",
      "3.  **Context Management Strategies:** Since the context window is limited, the protocol includes methods for handling long interactions or large amounts of data:\n",
      "    *   **Truncation:** Simple cutting off of older parts of the conversation history once the context window limit is approached.\n",
      "    *   **Summarization:** Condensing older parts of the conversation or long documents to save tokens.\n",
      "    *   **Sliding Window:** Keeping the most recent conversation turns and progressively removing the oldest ones.\n",
      "    *   **Retrieval-Augmented Generation (RAG):** A sophisticated strategy where relevant external information (from documents, databases, etc.) is *retrieved* based on the user's query and then inserted into the prompt as context *before* the model generates a response. This allows the model to access knowledge beyond its training data without having to fit entire external knowledge bases into the context window.\n",
      "4.  **Providing Relevant Information:** The protocol dictates that the most *relevant* information should be prioritized and included within the context window. This could be recent conversation turns, key instructions, or retrieved external data.\n",
      "5.  **Instruction and Data Separation:** Often, the protocol suggests separating core instructions (e.g., \"Act as a helpful assistant\") from the conversational history or the data being processed to ensure the instructions are always considered.\n",
      "6.  **Handling Input/Output Interfaces:** How developers interact with the model's API. APIs often implement aspects of this protocol (e.g., requiring separate roles for messages like `system`, `user`, `assistant`).\n",
      "\n",
      "**In summary:**\n",
      "\n",
      "The \"Model Context Protocol\" is the set of established (though often model-specific or application-specific) practices and architectural choices for preparing the input (\"context\") fed into an LLM's limited processing window, ensuring the model has the necessary information (instructions, history, relevant data) to generate a useful and coherent response. It addresses the fundamental challenge of providing dynamic, relevant information to a model with a fixed memory size.\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\"what is model context protocol\")\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
